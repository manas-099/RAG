{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1x-5pNdeCdCn"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install langchain langchain_community pypdf faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_75GNiF1D_YR"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install langchain_groq\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWTZVddSDFcA",
        "outputId": "532b17c4-277e-4a80-eac1-9dea74af90c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "page_content='MANNING\n",
            "Sebastian Raschka\n",
            "FROMSCRATCH\n",
            "BUILD A' metadata={'source': '/content/Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka-1-10.pdf', 'page': 0, 'page_label': '1'}\n"
          ]
        }
      ],
      "source": [
        "#loading the data\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "loader=PyPDFLoader(\"/content/Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka-1-10.pdf\")\n",
        "pages=loader.load()\n",
        "\n",
        "print(pages[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BjVqcx8cEKfN"
      },
      "outputs": [],
      "source": [
        "from langchain.docstore.document import Document\n",
        "all_text=\"\".join([page.page_content for page in pages])\n",
        "\n",
        "dataset=Document(page_content=all_text,metadata={\"source\":\"/content/Build_a_Large_Language_Model_From_Scratch_-_Sebastian_Raschka.pdf\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "nylro6tiN6R1",
        "outputId": "be838209-1c2a-4226-c391-5f92926839d3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5732"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(dataset.page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "izZF46HID7xd"
      },
      "outputs": [],
      "source": [
        "## chunking\n",
        "from langchain.text_splitter import CharacterTextSplitter,RecursiveCharacterTextSplitter\n",
        "text_splitter=RecursiveCharacterTextSplitter(chunk_size=600,chunk_overlap=200)\n",
        "chunks_data=text_splitter.split_documents([dataset])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApIXB6QKNvDb",
        "outputId": "9f44b34a-9f2a-4726-9d39-05a5e311b5e5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(chunks_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 369,
          "referenced_widgets": [
            "233d6bf7e2524efba5fa67c5e9f9bbaf",
            "d66291e85be840398ca872bbf1a50c13",
            "1d49a42aa70c431f890bcd86e9a2d0be",
            "4de0bd915dea46deb78fe5240d03cff6",
            "b1187509e796445791f90c8cbe563ed0",
            "fea0ef8656e04e95b14945e4359699c1",
            "fb4a191dbb5246d4977e7ebc2b6c2688",
            "67937c7438fc48c294443f7034cede14",
            "a4f1ae978fa4472ea67dc1bbb17335f3",
            "45dedcc3752a464c87763cd0402d9c73",
            "aaade745209842c2afc3eb2f162e9913"
          ]
        },
        "id": "Fy6IfWOCNaSx",
        "outputId": "cad9f689-585e-4eab-f8e0-2708b86c7145"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-17-717068d55aa3>:5: LangChainDeprecationWarning: The class `HuggingFaceBgeEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  hf_em_model=HuggingFaceBgeEmbeddings(\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "233d6bf7e2524efba5fa67c5e9f9bbaf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d66291e85be840398ca872bbf1a50c13",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1d49a42aa70c431f890bcd86e9a2d0be",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4de0bd915dea46deb78fe5240d03cff6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b1187509e796445791f90c8cbe563ed0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fea0ef8656e04e95b14945e4359699c1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fb4a191dbb5246d4977e7ebc2b6c2688",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "67937c7438fc48c294443f7034cede14",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a4f1ae978fa4472ea67dc1bbb17335f3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "45dedcc3752a464c87763cd0402d9c73",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aaade745209842c2afc3eb2f162e9913",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#creating vectorstor\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "model_name = \"sentence-transformers/all-MiniLM-L6-v2\" # Smaller model\n",
        "hf_em_model=HuggingFaceBgeEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs={\"device\":\"cpu\"},\n",
        "    encode_kwargs={\"normalize\":True}\n",
        ")\n",
        "vector_store=FAISS.from_documents(\n",
        "    documents=chunks_data,\n",
        "    embedding=hf_em_model\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sqUrCtXxRGkH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"GROQ_API_KEY\"]=\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "M7nYWuErHpaQ"
      },
      "outputs": [],
      "source": [
        "## multi query\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "template=\"\"\"\n",
        "You are an AI assistant that generate multiple question based on the user's input\n",
        "query or question. Generate multiple queries such that the questions are caputured the\n",
        "what the question is asking about in a different perspective.\n",
        "generate 5 queries in new line each based on user input query:\n",
        "{question}\n",
        "output 5 questions\n",
        "\n",
        "\"\"\"\n",
        "prompt=ChatPromptTemplate.from_template(template)\n",
        "\n",
        "llm=ChatGroq(\n",
        "    model=\"llama3-8b-8192\",\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "generate_queries_chain=(\n",
        "    prompt\n",
        "    |llm\n",
        "    |(lambda x: x.content.split(\"\\n\"))\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULitonGHR5AU",
        "outputId": "51a0370d-58f3-478a-95f7-654c7cdf70d5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Here are 5 questions that capture the essence of the user\\'s query \"what is LLM?\" from different perspectives:',\n",
              " '',\n",
              " 'What is the primary function of a Large Language Model (LLM) in natural language processing?',\n",
              " '',\n",
              " 'How does a Large Language Model (LLM) differ from other types of language models in terms of its architecture and capabilities?',\n",
              " '',\n",
              " 'What are some of the key applications and use cases of Large Language Models (LLMs) in industries such as customer service, content creation, and language translation?',\n",
              " '',\n",
              " 'Can a Large Language Model (LLM) be trained to perform specific tasks such as text classification, sentiment analysis, and question answering, and if so, how?',\n",
              " '',\n",
              " 'What are the potential limitations and challenges of using Large Language Models (LLMs) in real-world applications, and how can these be addressed through further research and development?']"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "question=\"what is llm?\"\n",
        "questions=generate_queries_chain.invoke(question)\n",
        "questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_I0TmWHPV4UA"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
