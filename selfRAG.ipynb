{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        " ! pip install -U langchain_community tiktoken  langchainhub chromadb  langgraph langchain_groq"
      ],
      "metadata": {
        "id": "pisda4sCMxLF"
      },
      "id": "pisda4sCMxLF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "64063aec",
      "metadata": {
        "id": "64063aec"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "565a6d44-2c9f-4fff-b1ec-eea05df9350d",
      "metadata": {
        "id": "565a6d44-2c9f-4fff-b1ec-eea05df9350d"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "\n",
        "urls = [\n",
        "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
        "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
        "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
        "]\n",
        "\n",
        "docs = [WebBaseLoader(url).load() for url in urls]\n",
        "docs_list = [item for sublist in docs for item in sublist]\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=250, chunk_overlap=0\n",
        ")\n",
        "doc_splits = text_splitter.split_documents(docs_list)\n",
        "\n",
        "model_name = \"BAAI/bge-small-en\"\n",
        "model_kwargs = {\"device\": \"cpu\"}\n",
        "encode_kwargs = {\"normalize_embeddings\": True}\n",
        "hf_embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
        ")\n",
        "\n",
        "# Add to vectorDB\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=doc_splits,\n",
        "    collection_name=\"rag-chroma\",\n",
        "    embedding=hf_embeddings,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "dcd77cc1-4587-40ec-b633-5364eab9e1ec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcd77cc1-4587-40ec-b633-5364eab9e1ec",
        "outputId": "94c23717-58df-48df-bc21-6961968a5577"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:253: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In a LLM-powered autonomous agent system, short-term memory is used for in-context learning, while long-term memory allows the agent to retain and recall information over extended periods, often through an external vector store and fast retrieval.\n"
          ]
        }
      ],
      "source": [
        "### Generate\n",
        "\n",
        "from langchain import hub\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Prompt\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "# LLM\n",
        "llm = ChatGroq(temperature=0)\n",
        "\n",
        "# Post-processing\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "# Chain\n",
        "rag_chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "# Run\n",
        "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
        "print(generation)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_4qmF8AtuM7w"
      },
      "id": "_4qmF8AtuM7w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#retriever node\n",
        "\n",
        "\n",
        "from langchain import hub\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "retriever = vectorstore.as_retriever()\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "retriever_chain=prompt|llm|StrOutputParser()\n"
      ],
      "metadata": {
        "id": "LEeNBf42RMEt"
      },
      "id": "LEeNBf42RMEt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question=\" What is prompt engineering?\"\n",
        "#retrieval node\n",
        "retrieved_docs=retriever.get_relevant_documents(question)\n",
        "doc_txt = retrieved_docs[1].page_content\n",
        "#Grading node means here we check relevance or not\n",
        "\n",
        "from pydantic import BaseModel,Field\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "class check_relevance(BaseModel):\n",
        "  binary_score:str=Field(description=\"generated_data is relevant,'yes',or,'no'\")\n",
        "\n",
        "llm = ChatGroq(\n",
        "    model=\"llama3-8b-8192\",\n",
        "    temperature=0)\n",
        "llm_grader_relevance=llm.with_structured_output(check_relevance)\n",
        "\n",
        "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n",
        "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
        "    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
        "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
        "relevance_prompt=ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\",system),\n",
        "        (\"human\",\"Retrieved documents \\n\\n:{documents} \\n\\n user question \\n\\n:{question}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "relevance_check_grader=relevance_prompt|llm_grader_relevance\n",
        "relevance_check_grader.invoke({\"documents\":retrieved_docs,\"question\":question})\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmcLMn3PR0D_",
        "outputId": "8b096aaf-796f-42e6-8799-0ad8b46db03a"
      },
      "id": "vmcLMn3PR0D_",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "check_relevance(binary_score='yes')"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#generae node and cheking hallucination\n",
        "\n",
        "class hallucination_check(BaseModel):\n",
        "  binary_score:str=Field(description=\"generated_data is graunded to the fact,'yes',or,'no'\")\n",
        "system = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n\n",
        "     Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\"\"\"\n",
        "llm_grader_hallucinate=llm.with_structured_output(hallucination_check)\n",
        "hallucinate_prompt=ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\",system),\n",
        "        (\"human\",\"generated documents \\n\\n:{generated_data} \\n\\n retrieved documents \\n\\n:{documents}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "genereted_data=rag_chain.invoke({\"context\": retrieved_docs, \"question\": question})\n",
        "\n",
        "hallucinate_check_grader=hallucinate_prompt|llm_grader_hallucinate\n",
        "hallucinate_check_grader.invoke({\"generated_data\":genereted_data,\"documents\":retrieved_docs})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQxZGF3CU6n_",
        "outputId": "366f0c95-d5f2-423d-c084-b6f00f3bca30"
      },
      "id": "RQxZGF3CU6n_",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "hallucination_check(binary_score='no')"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check the ans is address the question or not\n",
        "class check_addressing(BaseModel):\n",
        "  binary_score:str=Field(description=\"generated answer is addressing the question,'yes',or,'no'\")\n",
        "llm_grader_addressing=llm.with_structured_output(check_addressing)\n",
        "system = \"\"\"You are a grader assessing whether an answer addresses / resolves a question \\n\n",
        "     Give a binary score 'yes' or 'no'. Yes' means that the answer resolves the question.\"\"\"\n",
        "\n",
        "addressing_prompt=ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\",system),\n",
        "        (\"human\",\"generated documents \\n\\n:{generated_data} \\n\\n question \\n\\n:{question}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "# genereted_data=rag_chain.invoke({\"context\": retrieved_docs, \"question\": question})\n",
        "\n",
        "addressing_check_grader=addressing_prompt|llm_grader_addressing\n",
        "addressing_check_grader.invoke({\"generated_data\":genereted_data,\"question\":question})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZF1gX6bXENO",
        "outputId": "6d9ced41-de7b-482a-af2d-6fa4e371e234"
      },
      "id": "8ZF1gX6bXENO",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "check_addressing(binary_score='yes')"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# rewriting the query for better retrieval\n",
        "system = \"\"\"You a question re-writer that converts an input question to a better version that is optimized \\n\n",
        "     for vectorstore retrieval. Look at the input and try to reason about the underlying sematic intent / meaning. Just say the question. I dont need any explanation just question is enough\"\"\"\n",
        "\n",
        "\n",
        "rewriting_prompt=ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\",system),\n",
        "        (\"human\",\"  user  question \\n\\n:{question}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "question_rewriter=rewriting_prompt|llm|StrOutputParser()\n",
        "question_rewriter.invoke({\"question\":question})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "htRjV-k_ZAfU",
        "outputId": "e33422a2-6362-4f37-f914-a47b060ffce9"
      },
      "id": "htRjV-k_ZAfU",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What are the techniques used to design and optimize natural language prompts for effective model performance?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# graph state\n",
        "\n",
        "from typing_extensions import TypedDict\n",
        "from typing import List\n",
        "class GraphState(TypedDict):\n",
        "  question:str\n",
        "  generated_data:str\n",
        "  documents:List[str]\n",
        "\n"
      ],
      "metadata": {
        "id": "o826jW0lZz9S"
      },
      "id": "o826jW0lZz9S",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#nodes\n",
        "def retriever(state):\n",
        "  print(\"--Retriever_node---\")\n",
        "  question=state['question']\n",
        "  documents=state['documents']\n",
        "  retriever = vectorstore.as_retriever()\n",
        "  retrieved_docs=retriever.get_relevant_documents(question)\n",
        "  return {'documents':retrieved_docs,\"question\":question}\n",
        "\n",
        "\n",
        "def relevance_check_grade(state):\n",
        "  documents=state['documents']\n",
        "  question=state['question']\n",
        "  score=relevance_check_grader.invoke({\"documents\":documents,\"question\":question})\n",
        "  grade=score.binary_score\n",
        "  filter_docs=[]\n",
        "  for doc in documents:\n",
        "      score=relevance_check_grader.invoke({\"documents\":doc.page_content,\"question\":question})\n",
        "      grade=score.binary_score\n",
        "      if grade=='yes':\n",
        "        print(\"--Grade:__Document_relevant_----\")\n",
        "        filter_docs.append(doc)\n",
        "      else:\n",
        "        print(\"--Grade:__Document_not_relevant__----\")\n",
        "        continue\n",
        "  return {\"documents\": filter_docs, \"question\": question}\n",
        "\n",
        "def generate(state):\n",
        "  print(\"Generate ans grade \")\n",
        "  documents=state['documents']\n",
        "  question=state['question']\n",
        "  generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
        "  return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
        "def transform_query(state):\n",
        "  print(\"--node rewriting the query---\")\n",
        "  question=state['question']\n",
        "  new_question=question_rewriter.invoke({\"question\":question})\n",
        "  return {\"question\":new_question}\n",
        "def decide_to_generate(state):\n",
        "  print(\"------ASSESS GRADED DOCUMENTS-----\")\n",
        "  question=state['question']\n",
        "  filtered_documents = state[\"documents\"]\n",
        "  if not filtered_documents:\n",
        "    print(\"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\")\n",
        "    return \"transform_query\"\n",
        "  else:\n",
        "    print(\"---DECISION: DOCUMENTS ARE RELEVANT TO QUESTION,GENERATE--\")\n",
        "    return \"generate\"\n",
        "\n",
        "# def hallucinate_check_grade(state):\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def hallucinate_check_grade(state):\n",
        "    print(\"---CHECK HALLUCINATIONS---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "    generation = state[\"generation\"]\n",
        "\n",
        "    # Change 'generade_data' to 'generated_data'\n",
        "    score=hallucinate_check_grader.invoke({\"generated_data\":generation,\"documents\":documents})\n",
        "    grade=score.binary_score\n",
        "    if grade == 'yes':\n",
        "      print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
        "      # Check question-answering\n",
        "      print(\"---GRADE GENERATION vs QUESTION---\")\n",
        "      score=addressing_check_grader.invoke({\"generated_data\":genereted_data,\"question\":question})\n",
        "      grade=score.binary_score\n",
        "      if grade=='yes':\n",
        "        print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
        "        return \"usefull\"\n",
        "      else:\n",
        "        print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
        "        return \"not useful\"\n",
        "    else:\n",
        "      print(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
        "      return \"not supported\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "q9ba7Yxwajxp"
      },
      "id": "q9ba7Yxwajxp",
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#graph nodes\n",
        "from langgraph.graph import END, StateGraph\n",
        "workflow=StateGraph(GraphState)\n",
        "\n",
        "workflow.add_node(\"retriever\",retriever)\n",
        "workflow.add_node(\"generate\",generate)\n",
        "workflow.add_node(\"relevance_check_grade\",relevance_check_grade)\n",
        "workflow.add_node(\"transform_query\",transform_query)\n",
        "\n",
        "\n",
        "workflow.set_entry_point(\"retriever\")\n",
        "workflow.add_edge(\"retriever\",'relevance_check_grade')\n",
        "workflow.add_conditional_edges(\n",
        "    \"relevance_check_grade\",\n",
        "   decide_to_generate,\n",
        "    {\n",
        "    \"transform_query\":\"transform_query\",\n",
        "    \"generate\":\"generate\",\n",
        "\n",
        "}\n",
        "    )\n",
        "workflow.add_edge(\"transform_query\",\"retriever\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"generate\",\n",
        "    hallucinate_check_grade,\n",
        "     {\n",
        "\n",
        "    \"usefull\":END,\n",
        "    \"not useful\":\"transform_query\",\n",
        "    \"not supported\":\"generate\"\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VLfDxfQaOpm",
        "outputId": "5eca8675-44fa-4ac8-f39b-a49d851866b5"
      },
      "id": "_VLfDxfQaOpm",
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7cbcbce4d7d0>"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "app=workflow.compile()"
      ],
      "metadata": {
        "id": "ClF3HYa6xkAQ"
      },
      "id": "ClF3HYa6xkAQ",
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n"
      ],
      "metadata": {
        "id": "hHd3_56fyJDY"
      },
      "id": "hHd3_56fyJDY",
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = {\"question\": \"What is Self-Reflection?\", \"documents\": []}  # Add documents key with an empty list\n",
        "\n",
        "for output in app.stream(inputs):\n",
        "    for key, value in output.items():\n",
        "\n",
        "        pprint(f\"Node '{key}':\")\n",
        "\n",
        "    pprint(\"\\n---\\n\")\n",
        "\n",
        "pprint(value[\"generation\"])"
      ],
      "metadata": {
        "id": "fXztxWxI0qdi"
      },
      "id": "fXztxWxI0qdi",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}