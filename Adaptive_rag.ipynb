{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#adaptive rag=query analysis + self rag"
      ],
      "metadata": {
        "id": "yJbjsH9DA8U6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "ERQ_ZnotdWIb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install langchain_groq langchain_community tiktoken chromadb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdzPt3Z6dxK6",
        "outputId": "1b6c941c-e207-4ebe-ad2e-48bbed2f9e01"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.1/611.1 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.4/177.4 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.7/118.7 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.0/73.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m93.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Build Index\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "\n",
        "# Set embeddings\n",
        "model_name = \"BAAI/bge-small-en\"\n",
        "model_kwargs = {\"device\": \"cpu\"}\n",
        "\n",
        "encode_kwargs = {\"normalize_embeddings\": True}\n",
        "hf_embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
        ")\n",
        "\n",
        "# Docs to index\n",
        "urls = [\n",
        "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
        "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
        "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
        "]\n",
        "\n",
        "# Load\n",
        "docs = [WebBaseLoader(url).load() for url in urls]\n",
        "docs_list = [item for sublist in docs for item in sublist]\n",
        "\n",
        "# Split\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=500, chunk_overlap=0\n",
        ")\n",
        "doc_splits = text_splitter.split_documents(docs_list)\n",
        "\n",
        "# Add to vectorstore\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=doc_splits,\n",
        "    collection_name=\"rag-chroma\",\n",
        "    embedding=hf_embeddings,\n",
        ")\n",
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "VjIzSIj6d78H"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import field\n",
        "##Router\n",
        "from typing import Literal\n",
        "from pydantic import BaseModel,Field\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "class query_router(BaseModel):\n",
        "  datasource:Literal[\"web_search\",\"vectorstore\"]=Field(\n",
        "      ...,\n",
        "      description=\"Given a user's query choose to route the query  to vectorstore or web_search\"\n",
        "  )\n",
        "\n",
        "llm=ChatGroq(model=\"llama3-8b-8192\",temperature=0)\n",
        "stuctured_llm=llm.with_structured_output(query_router)\n",
        "\n",
        "system=\"\"\"\n",
        "You are an expert to route the user's query to vectorstore or web_search.\n",
        "the Vectorstore contains data related to ai agengts, prompt engineering, and adversarial attacks.\n",
        "use the vectorstore for questions on these topics. Otherwise, use web-search.\n",
        "\"\"\"\n",
        "\n",
        "prompt=ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\",system),\n",
        "        (\"human\",\"{query}\")\n",
        "\n",
        "    ]\n",
        ")\n",
        "\n",
        "choose_route_chain=prompt|stuctured_llm\n",
        "query=\"What is AI agent?\"\n",
        "choose_route_chain.invoke({\"query\":query})\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iw2j4KPSetkW",
        "outputId": "0a4355f3-e68c-4565-d77e-752c7d1fccbc"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "query_router(datasource='web_search')"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "choose_route_chain.invoke({\"query\":\"who is Elon Musk?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afsqHdP2hNr_",
        "outputId": "3f30129e-8fb9-4939-e05e-bd3ae8e148f2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "query_router(datasource='web_search')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#retriever node\n",
        "query=\"What is AI agent?\"\n",
        "retrieved_docs=retriever.get_relevant_documents(query)\n",
        "retrieved_data=retrieved_docs[1].page_content"
      ],
      "metadata": {
        "id": "QHfHDdzrk-Wv"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever.invoke(query)"
      ],
      "metadata": {
        "id": "lvzhZMtUrIPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate node\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "template=\"\"\"You are an Ai Assistant ,you job is to generate answer to user's query based on the context.\n",
        "here is the context:{context}\n",
        "here is the query:{query}\n",
        "answer:\"\"\"\n",
        "rag_prompt=ChatPromptTemplate.from_template(template)\n",
        "rag_chain=rag_prompt|llm|StrOutputParser()\n",
        "query=\"What is AI agent?\"\n",
        "generated_data=rag_chain.invoke({\"context\":retrieved_data,\"query\":query})\n",
        "generated_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "opOtMBcDhhnW",
        "outputId": "7c00b80d-2688-4030-88ea-8e861357cc31"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Based on the provided context, I'd be happy to help!\\n\\nAccording to the context, an AI agent is a type of autonomous agent that uses a Large Language Model (LLM) as its main controller. This AI agent is designed to make decisions independently without seeking user assistance, and it has a short-term memory limit of approximately 4000 words. The agent's primary goal is to optimize believability at the moment versus in time, taking into account relationships between agents and observations of one agent by another.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "wVwqob8ql4KE",
        "outputId": "85e41aa4-69be-43c0-c13b-ce4cd64b008c"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Planning is essentially in order to optimize believability at the moment vs in time.\\nPrompt template: {Intro of an agent X}. Here is X\\'s plan today in broad strokes: 1)\\nRelationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\\nEnvironment information is present in a tree structure.\\n\\n\\n\\n\\nFig. 13. The generative agent architecture. (Image source: Park et al. 2023)\\nThis fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others).\\nProof-of-Concept Examples#\\nAutoGPT has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\\nHere is the system message used by AutoGPT, where {{...}} are user inputs:\\nYou are {{ai-name}}, {{user-provided AI bot description}}.\\nYour decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\\n\\nGOALS:\\n\\n1. {{user-provided goal 1}}\\n2. {{user-provided goal 2}}\\n3. ...\\n4. ...\\n5. ...\\n\\nConstraints:\\n1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\\n2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\\n3. No user assistance\\n4. Exclusively use the commands listed in double quotes e.g. \"command name\"\\n5. Use subprocesses for commands that will not terminate within a few minutes'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# relevance or not checking grade\n",
        "\n",
        "class grade_documents(BaseModel):\n",
        "  binary_score:str=Field(\n",
        "      description=(\"retrieved documents are relevant,'yes',or,'no'\")\n",
        "\n",
        "  )\n",
        "relevance_llm=llm.with_structured_output(grade_documents)\n",
        "system=\"\"\"\n",
        "You are a grader assessing relevance of a retrieved document to a user question. \\n\n",
        "    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
        "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
        "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\n",
        "\"\"\"\n",
        "relevance_prompt=ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\",system),\n",
        "        (\"human\",\"retrieved_documents:\\n\\n{documents}\\nquery:\\n\\n{query}\")\n",
        "    ]\n",
        ")\n",
        "relevance_grader=relevance_prompt|relevance_llm\n",
        "query=\"What is AI agent?\"\n",
        "retrieved_docs=retriever.get_relevant_documents(query)\n",
        "retrieved_data=retrieved_docs[1].page_content\n",
        "print(retrieved_data)\n",
        "score=relevance_grader.invoke({\"documents\":retrieved_data,\"query\":query})\n",
        "score.binary_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 576
        },
        "id": "bVz_S0kKivp7",
        "outputId": "a25060fc-61c1-4139-df6c-512805aace90"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Planning is essentially in order to optimize believability at the moment vs in time.\n",
            "Prompt template: {Intro of an agent X}. Here is X's plan today in broad strokes: 1)\n",
            "Relationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\n",
            "Environment information is present in a tree structure.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fig. 13. The generative agent architecture. (Image source: Park et al. 2023)\n",
            "This fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others).\n",
            "Proof-of-Concept Examples#\n",
            "AutoGPT has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\n",
            "Here is the system message used by AutoGPT, where {{...}} are user inputs:\n",
            "You are {{ai-name}}, {{user-provided AI bot description}}.\n",
            "Your decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\n",
            "\n",
            "GOALS:\n",
            "\n",
            "1. {{user-provided goal 1}}\n",
            "2. {{user-provided goal 2}}\n",
            "3. ...\n",
            "4. ...\n",
            "5. ...\n",
            "\n",
            "Constraints:\n",
            "1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\n",
            "2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\n",
            "3. No user assistance\n",
            "4. Exclusively use the commands listed in double quotes e.g. \"command name\"\n",
            "5. Use subprocesses for commands that will not terminate within a few minutes\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'yes'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# hallucinate grader\n",
        "\n",
        "class hallucinate_checker(BaseModel):\n",
        "  binary_score:str=Field(\n",
        "      description=(\"Answer is grounded to facts ,'yes',or,'no'\")\n",
        "  )\n",
        "\n",
        "hallucinate_llm=llm.with_structured_output(hallucinate_checker)\n",
        "system = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n\n",
        "     Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\"\"\"\n",
        "hallucinate_prompt=ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\",system),\n",
        "        (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generated_data}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "hallucinate_grader=hallucinate_prompt|hallucinate_llm\n",
        "\n",
        "score=hallucinate_grader.invoke({\"documents\":retrieved_data,\"generated_data\":generated_data})\n",
        "score.binary_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "n13DOAvMktw7",
        "outputId": "4dc0d0d4-9add-43bc-e0a0-ecd4d1771bcb"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'yes'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# question rewriter\n",
        "# Prompt\n",
        "system = \"\"\"You a question re-writer that converts an input question to a better version that is optimized \\n\n",
        "     for vectorstore retrieval. Look at the input and try to reason about the underlying sematic intent / meaning. Just provide the new question alone no need for any other content.\"\"\"\n",
        "re_write_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "question_rewriter = re_write_prompt | llm | StrOutputParser()\n",
        "query=\"What is AI agent?\"\n",
        "question_rewriter.invoke({\"question\": query})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "P6O3aS2Co8Xr",
        "outputId": "dc1200f5-ef13-4f00-fc06-3320ac291df8"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What are the characteristics and functionalities of an artificial intelligence agent?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Answer Grader\n",
        "\n",
        "\n",
        "class GradeAnswer(BaseModel):\n",
        "\n",
        "\n",
        "    binary_score: str = Field(description=\"Answer addresses the question, 'yes' or 'no'\")\n",
        "\n",
        "\n",
        "\n",
        "structured_llm_grader = llm.with_structured_output(GradeAnswer)\n",
        "\n",
        "\n",
        "system = \"\"\"You are a grader assessing whether an answer addresses / resolves a question \\n\n",
        "     Give a binary score 'yes' or 'no'. Yes' means that the answer resolves the question.\"\"\"\n",
        "answer_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generated_data}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "answer_grader = answer_prompt | structured_llm_grader\n",
        "score=answer_grader.invoke({\"question\": query,\"generated_data\": generated_data})\n",
        "score.binary_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "shwBBvqOpQHE",
        "outputId": "3eedff6c-13aa-4b9b-eb11-1ed6f2ad0e52"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'no'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#web search\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "web_search_tool = TavilySearchResults(k=3)"
      ],
      "metadata": {
        "id": "ka4LYdpVtM48"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# graph state\n",
        "from typing_extensions import TypedDict\n",
        "from typing import List\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "    query:str\n",
        "    generated_data:str\n",
        "    documents:List[str]\n",
        "\n"
      ],
      "metadata": {
        "id": "LZ7ZKfEypoby"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install langgraph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpfQZxQVqQGK",
        "outputId": "a604963f-02b8-4aa4-8d51-e7711c83229a"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/150.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.2/150.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.7/44.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#graph flow define\n",
        "from langgraph.graph import END, StateGraph\n",
        "from langchain.schema import Document\n",
        "\n",
        "#nodes\n",
        "def retrieve(state):\n",
        "  print(\"---RETRIEVE---\")\n",
        "  query=state['query']\n",
        "\n",
        "  documents=retriever.invoke(query)\n",
        "  return {\"documents\":documents,\"query\":query}\n",
        "\n",
        "\n",
        "\n",
        "def generate(state):\n",
        "  print(\"---GENERATE---\")\n",
        "  query=state['query']\n",
        "  retrieved_data=state['documents']\n",
        "  generated_data=rag_chain.invoke({\"context\":retrieved_data,\"query\":query})\n",
        "\n",
        "  return {\"documents\":retrieved_data,\"query\":query,\"generated_data\":generated_data}\n",
        "\n",
        "def gradeing_docs(state):\n",
        "  print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
        "  query=state['query']\n",
        "  documents=state['documents']\n",
        "  filtered_docs=[]\n",
        "  for doc in documents:\n",
        "    score=relevance_grader.invoke({\"documents\":doc.page_content,\"query\":query})\n",
        "    grade=score.binary_score\n",
        "    if grade=='yes':\n",
        "      print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
        "      filtered_docs.append(doc)\n",
        "    else:\n",
        "      print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
        "      continue\n",
        "  return {\"documents\": filtered_docs, \"query\": query}\n",
        "\n",
        "def transform_query(state):\n",
        "\n",
        "\n",
        "    print(\"---TRANSFORM QUERY---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    better_question = question_rewriter.invoke({\"question\": question})\n",
        "    return {\"documents\": documents, \"question\": better_question}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def web_search(state):\n",
        "\n",
        "\n",
        "    print(\"---WEB SEARCH---\")\n",
        "    query = state[\"query\"]\n",
        "\n",
        "    # Web search\n",
        "    docs = web_search_tool.invoke({\"query\": query})\n",
        "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
        "    web_results = Document(page_content=web_results)\n",
        "\n",
        "    return {\"documents\": web_results, \"query\": query}\n",
        "\n",
        "\n",
        "def searching_route(state):\n",
        "  print(\"---ROUTE QUESTION---\")\n",
        "  query=state['query']\n",
        "  # docs = web_search_tool.invoke({\"query\": question})\n",
        "  res=choose_route_chain.invoke({\"query\":query})\n",
        "  if res.datasource=='web_search':\n",
        "    print(\"---ROUTE QUESTION TO WEB SEARCH---\")\n",
        "    return \"web_search\"\n",
        "  else:\n",
        "    print(\"---ROUTE QUESTION TO RAG---\")\n",
        "    return \"vectorstore\"\n",
        "\n",
        "\n",
        "def decide_to_generate(state):\n",
        "  print(\"---ASSESS GRADED DOCUMENTS---\")\n",
        "  query = state[\"query\"]\n",
        "  filtered_documents = state[\"documents\"]\n",
        "  if not filtered_documents:\n",
        "    print(\"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\")\n",
        "    return \"transform_query\"\n",
        "  else:\n",
        "    print(\"---DECISION: GENERATE---\")\n",
        "    return \"generate\"\n",
        "\n",
        "def hallucinate_check(state):\n",
        "  print(\"---CHECK HALLUCINATIONS---\")\n",
        "  query=state[\"query\"]\n",
        "  generated_data=state['generated_data']\n",
        "  documents = state[\"documents\"]\n",
        "\n",
        "  score=hallucinate_grader.invoke({\"documents\":documents,\"generated_data\":generated_data})\n",
        "  grade=score.binary_score\n",
        "  if grade=='yes':\n",
        "    print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
        "    print(\"Grade--checking ans is addresses the query---\")\n",
        "    score=answer_grader.invoke({\"question\": query,\"generated_data\": generated_data})\n",
        "    grade=score.binary_score\n",
        "    if grade==\"yes\":\n",
        "      print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
        "\n",
        "      return \"useful\"\n",
        "    else:\n",
        "      print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
        "      return \"not useful\"\n",
        "  else:\n",
        "    print(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
        "    return \"not supported\"\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7tD8eqQpqLj3"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "workflow=StateGraph(GraphState)\n",
        "\n",
        "#adding nodes\n",
        "workflow.add_node(\"web_search\", web_search)\n",
        "workflow.add_node(\"retrieve\",retrieve)\n",
        "workflow.add_node(\"gradeing_docs\", gradeing_docs)\n",
        "workflow.add_node(\"generate\", generate)\n",
        "workflow.add_node(\"transform_query\", transform_query)\n",
        "\n",
        "#edges\n",
        "\n",
        "workflow.set_conditional_entry_point(\n",
        "    searching_route,\n",
        "    {\n",
        "        \"web_search\":\"web_search\",\n",
        "        \"vectorstore\":\"retrieve\"\n",
        "    }\n",
        ")\n",
        "workflow.add_edge(\"web_search\",\"generate\")\n",
        "workflow.add_edge(\"retrieve\",\"gradeing_docs\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"gradeing_docs\",\n",
        "    decide_to_generate,\n",
        "    {\n",
        "        \"transform_query\":\"transform_query\",\n",
        "        \"generate\":\"generate\"\n",
        "    }\n",
        ")\n",
        "workflow.add_edge(\"transform_query\", \"retrieve\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"generate\",\n",
        "    hallucinate_check,\n",
        "    {\n",
        "        \"useful\":END,\n",
        "        \"not useful\":\"transform_query\",\n",
        "        \"not supported\":\"generate\"\n",
        "    }\n",
        "\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cwr-hn44MM_",
        "outputId": "39df99f6-85d4-4a18-dfb7-e2f41d7b4ebb"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x78379df0f7d0>"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "app=workflow.compile()"
      ],
      "metadata": {
        "id": "njVjk3K46irz"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "input={\"query\":\"what is ai agent memory?\",\"documents\":[]}\n",
        "for output in app.stream(input):\n",
        "  for key,value in output.items():\n",
        "    print(f\"node---- : {key}\")\n",
        "  print(\"\\n--\\n\")\n",
        "print(value['generated_data'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USpMspzh6laD",
        "outputId": "dfa7046c-3d4b-42aa-fb39-087cfb934c2a"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---ROUTE QUESTION---\n",
            "---ROUTE QUESTION TO RAG---\n",
            "---RETRIEVE---\n",
            "node---- : retrieve\n",
            "\n",
            "--\n",
            "\n",
            "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---ASSESS GRADED DOCUMENTS---\n",
            "---DECISION: GENERATE---\n",
            "node---- : gradeing_docs\n",
            "\n",
            "--\n",
            "\n",
            "---GENERATE---\n",
            "---CHECK HALLUCINATIONS---\n",
            "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
            "Grade--checking ans is addresses the query---\n",
            "---DECISION: GENERATION ADDRESSES QUESTION---\n",
            "node---- : generate\n",
            "\n",
            "--\n",
            "\n",
            "According to the provided context, AI agent memory refers to the ability of an LLM-powered autonomous agent to retain and recall information over extended periods. This is achieved through the use of an external vector store and fast retrieval mechanism, which enables the agent to access and utilize its past experiences and knowledge.\n",
            "\n",
            "In the context of the LLM-powered autonomous agent system, there are two types of memory:\n",
            "\n",
            "1. **Short-term memory**: This refers to the agent's ability to learn and utilize information in the context of a specific task or scenario. This is achieved through in-context learning and is closely related to the agent's planning and reflection mechanisms.\n",
            "2. **Long-term memory**: This refers to the agent's ability to retain and recall information over extended periods, often by leveraging an external vector store and fast retrieval mechanism. This enables the agent to access and utilize its past experiences and knowledge to inform its future behavior.\n",
            "\n",
            "In addition, the context also mentions the concept of a \"memory stream\", which is a long-term memory module that records a comprehensive list of agents' experiences in natural language. This memory stream is used to inform the agent's behavior and guide its future actions.\n",
            "\n",
            "Overall, AI agent memory is a critical component of the LLM-powered autonomous agent system, enabling the agent to learn, adapt, and make decisions based on its past experiences and knowledge.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input={\"query\":\"what is ai agent?\",\"documents\":[]}\n",
        "for output in app.stream(input):\n",
        "  for key,value in output.items():\n",
        "    print(f\"node---- : {key}\")\n",
        "  print(\"\\n--\\n\")\n",
        "print(value['generated_data'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4h9koYm7QtU",
        "outputId": "03ecb38a-4b58-4c46-ec8f-2f4bb5e04236"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---ROUTE QUESTION---\n",
            "---ROUTE QUESTION TO WEB SEARCH---\n",
            "---WEB SEARCH---\n",
            "node---- : web_search\n",
            "\n",
            "--\n",
            "\n",
            "---GENERATE---\n",
            "---CHECK HALLUCINATIONS---\n",
            "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
            "Grade--checking ans is addresses the query---\n",
            "---DECISION: GENERATION ADDRESSES QUESTION---\n",
            "node---- : generate\n",
            "\n",
            "--\n",
            "\n",
            "An artificial intelligence (AI) agent is a software program that can interact with its environment, collect data, and use the data to perform self-determined tasks to meet predetermined goals.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SZY_3lMB-r7-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}